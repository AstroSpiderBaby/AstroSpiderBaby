Welcome to my GitHub! I specialize in building modern, modular data pipelines with **Databricks**, **Azure**, and **Delta Lake**â€”with a focus on clean design, automation, and secure architecture.

---

ğŸ”¥ Featured Project: Modular Databricks Pipeline Framework

This project demonstrates an end-to-end batch ingestion pipeline using **Azure Databricks**, **Delta Lake**, and **Azure Key Vault**â€”with a clean Bronze â†’ Silver â†’ Gold architecture, secure mounting, and workflow orchestration.

ğŸ§ª **Includes full mock datasets** for easy testing and demonstration  
ğŸ“¦ [`mock_data/`](https://github.com/AstroSpiderBaby/databricks-pipelines/tree/main/mock_data) â€” CSV & JSON files used for Bronze layer ingestion  
ğŸ› ï¸ [`generate_mock_data.py`](https://github.com/AstroSpiderBaby/databricks-pipelines/blob/main/generate_mock_data.py) â€” Optional script to re-export files from Delta or Blob

ğŸ”— **Live Repo:** [databricks-pipelines](https://github.com/AstroSpiderBaby/databricks-pipelines)  
ğŸ“˜ **Full Docs:** [Root README](https://github.com/AstroSpiderBaby/databricks-pipelines#-databricks-pipelines)

---

## ğŸ“‘ Table of Contents

- [ğŸ”§ Current Capabilities](#-current-capabilities)
- [ğŸš€ Whatâ€™s Next](#-whats-next)
- [ğŸ“ Project Overview](#-project-overview)
- [ğŸ§  About Me](#-about-me)

---

## ğŸ”§ Current Capabilities

- ğŸ—ï¸ Modular pipeline structure using `bronze/`, `silver/`, and `gold/` stages
- ğŸ” Secure integration with **Azure Key Vault** and **Databricks Secret Scopes**
- ğŸ”„ SQL Server (on-prem) to Databricks via **Ngrok + JDBC**  
- ğŸ“¦ Blob storage mounting for ingestion of flat files and JSON
- ğŸ“‹ Databricks Workflows orchestrating multi-step pipelines with task dependencies
- ğŸ§© PySpark transformations for clean enrichment, joins, and validations
- ğŸªµ Delta format tables and Databricks Workflows for orchestration
- ğŸ§ª Full pipeline run documented via notebooks (and CLI support)

---

## ğŸš€ Whatâ€™s Next

- ğŸŒ€ Auto Loader integration for real-time ingestion  
- ğŸ” Incremental ingestion patterns using file metadata or watermark fields  
- ğŸ” Metadata-driven configs via YAML for flexible pipeline control  
- ğŸ§ª Layer-specific data quality checks:
  - **Bronze**: Schema enforcement, null validation, corrupt record handling  
  - **Silver**: Deduplication, referential integrity, outlier detection  
  - **Gold**: Metric validation, completeness checks, threshold alerts  
- ğŸ“¦ S3 integration for cross-cloud ingestion from AWS sources  
- ğŸ› ï¸ Alternate pipeline with **DBT** for declarative transformations and model versioning  
- ğŸ“Š Dashboard-ready Gold Layer with KPIs for compliance & vendor health


---

## ğŸ“ Project Overview

```text
pipeline1_batch_delta/
â”œâ”€â”€ bronze/       # Ingest mock data (finance, vendors, etc.)
â”œâ”€â”€ silver/       # Clean & join (e.g., vendor compliance)
â”œâ”€â”€ gold/         # Final aggregated outputs
â”œâ”€â”€ utils/        # Mounts, secrets, helpers
â”œâ”€â”€ transform/    # Modular transformation logic
â”œâ”€â”€ docs/         # Setup instructions and scripts
```

---
ğŸ’¬ About Me  
I'm a data engineer passionate about building scalable data systems using open-source tools and cloud-native architectures.

ğŸ“œ Certified: [Databricks Certified Data Engineer Associate](https://credentials.databricks.com/5c463e68-5e4a-41e0-a127-3f9c8501c68e#acc.xCdZN3jg)  

ğŸŒ Letâ€™s connect â†’ [linkedin.com/in/brucejenks](https://linkedin.com/in/brucejenks)  

ğŸ”„ More coming soon: dbt, streaming, governance, and advanced SQL tuning!


ğŸ§ª Maintained by AstroSpiderBaby
ğŸ•’ Last updated: July 2025
"""
